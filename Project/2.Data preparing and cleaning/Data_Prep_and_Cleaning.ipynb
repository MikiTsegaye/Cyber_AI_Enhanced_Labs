{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04b053c3-8fa9-4f80-868b-d5dbbbf735f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\noaga\\miniconda3\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.12.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\noaga\\miniconda3\\lib\\site-packages (8.1.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from ipywidgets) (9.7.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: colorama>=0.4.4 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator>=4.3.2 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: umap-learn in c:\\users\\noaga\\miniconda3\\lib\\site-packages (0.5.9.post2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\noaga\\miniconda3\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in c:\\users\\noaga\\miniconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from umap-learn) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.3.1 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from umap-learn) (1.16.3)\n",
      "Requirement already satisfied: scikit-learn>=1.6 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from umap-learn) (1.7.2)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from umap-learn) (0.63.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from umap-learn) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn) (0.46.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from pynndescent>=0.5->umap-learn) (1.5.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from scikit-learn>=1.6->umap-learn) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\noaga\\miniconda3\\lib\\site-packages (from tqdm->umap-learn) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install ipywidgets\n",
    "!pip install umap-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2f4f97f-29de-450d-b954-5fb0c958c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PHASE 3 IMPORTS ---\n",
    "\n",
    "# For data handling and loading the downloaded CSVs\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5860232-83dc-4b1b-a800-85d3fb12a078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Dataset Size: 447417 emails\n",
      "Phishing Dataset Size: 29767 emails\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Replace 'path/to/your/enron_data.csv' with the actual file path on your machine\n",
    "\n",
    "# Load the Normal (Training) Dataset\n",
    "df_normal = pd.read_csv(r'C:\\Users\\noaga\\OneDrive\\Desktop\\2025-6\\AI Enhanced cyber\\Project\\Real_data\\enron_data.csv',dtype = str)\n",
    "\n",
    "# Load the Phishing (Test) Dataset\n",
    "df_phishing_test = pd.read_csv(r'C:\\Users\\noaga\\OneDrive\\Desktop\\2025-6\\AI Enhanced cyber\\Project\\Real_data\\phishing_test_data.csv', dtype = str)\n",
    "\n",
    "print(f\"Normal Dataset Size: {df_normal.shape[0]} emails\")\n",
    "print(f\"Phishing Dataset Size: {df_phishing_test.shape[0]} emails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16fee747-950d-4906-a6ef-3dafbd8aa164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Normal Data...\n",
      "Preparing Phishing Test Data...\n",
      "Data combined successfully. Total emails: 477184\n"
     ]
    }
   ],
   "source": [
    "# --- 1. PREPARE NORMAL DATA (df_normal) ---\n",
    "print(\"Preparing Normal Data...\")\n",
    "# Select the Subject, Body, and the existing Label column from the Enron set\n",
    "# We rename the columns to lowercase standard names immediately\n",
    "df_normal_clean = df_normal.rename(columns={\n",
    "    'Subject': 'subject',\n",
    "    'Body': 'body',\n",
    "    'Label': 'old_label'  # Keep the old label for inspection, but we will create a new one\n",
    "})\n",
    "\n",
    "# Filter down to the essential content columns and the label\n",
    "df_normal_clean = df_normal_clean[['subject', 'body', 'old_label']]\n",
    "\n",
    "# Create the new, consistent 'is_phishing' label (0 = Normal)\n",
    "df_normal_clean['is_phishing'] = 0\n",
    "\n",
    "\n",
    "# --- 2. PREPARE PHISHING DATA (df_phishing_test) ---\n",
    "print(\"Preparing Phishing Test Data...\")\n",
    "# We assume the 'label' column in the phishing set already means 1=Phishing.\n",
    "# We rename the existing 'label' to 'is_phishing' for consistency.\n",
    "df_phishing_test = df_phishing_test.rename(columns={'label': 'is_phishing'})\n",
    "\n",
    "# Ensure the phishing data is labeled 1 (Phishing) and contains only the content columns\n",
    "df_phishing_test['is_phishing'] = df_phishing_test['is_phishing'].astype(int).replace(0, 1) # Ensure all attacks are 1\n",
    "\n",
    "\n",
    "# --- 3. MERGE THE DATASETS ---\n",
    "df_combined = pd.concat([df_normal_clean, df_phishing_test], ignore_index=True)\n",
    "print(f\"Data combined successfully. Total emails: {len(df_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c26dff6-6cb9-4b20-8019-99226b80bfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling NaN values in content columns...\n",
      "Applying cleaning and PII anonymization...\n",
      "Cleaning complete. Final dataset size: 477162\n"
     ]
    }
   ],
   "source": [
    "import re # Ensure this is also imported!\n",
    "\n",
    "def anonymize_and_clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs PII anonymization and standard text cleaning.\n",
    "    \"\"\"\n",
    "    # ... (the rest of the PII and cleaning logic)\n",
    "    # ...\n",
    "    return text.lower() # This entire block must be run!\n",
    "\n",
    "# --- 1. FILL MISSING VALUES (GUARANTEE NON-NULL INPUT) ---\n",
    "print(\"Filling NaN values in content columns...\")\n",
    "# Fill any missing values in 'subject' and 'body' with an empty string\n",
    "df_combined['subject'] = df_combined['subject'].fillna('')\n",
    "df_combined['body'] = df_combined['body'].fillna('')\n",
    "# Optional: Remove rows where both subject and body are empty after filling\n",
    "df_combined = df_combined[\n",
    "    (df_combined['subject'] != '') | (df_combined['body'] != '')\n",
    "].copy()\n",
    "\n",
    "# --- 2. APPLY CLEANING AND CONCATENATION (GUARANTEE STRING OUTPUT) ---\n",
    "print(\"Applying cleaning and PII anonymization...\")\n",
    "\n",
    "# Combine subject and body into one clean_text column\n",
    "df_combined['clean_text'] = df_combined.apply(\n",
    "    # We wrap the output of anonymize_and_clean_text with str() \n",
    "    # to guarantee the concatenation operator (+) only sees strings.\n",
    "    lambda row: str(anonymize_and_clean_text(row['subject'])) + \" \" + str(anonymize_and_clean_text(row['body'])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --- 3. FINAL SANITY CHECK ---\n",
    "# Remove any rows where cleaning resulted in only whitespace\n",
    "df_combined = df_combined[df_combined['clean_text'].str.strip() != '']\n",
    "\n",
    "print(f\"Cleaning complete. Final dataset size: {len(df_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31fbdbb4-6477-46d2-90f6-f44aedc783ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal emails isolated: 447395\n",
      "Phishing emails isolated: 29767\n",
      "\n",
      "--- Final Dataset Sizes ---\n",
      "Training Set (100% Normal): 313176 emails\n",
      "Validation Set (100% Normal): 67109 emails\n",
      "Test Set (Normal + Phishing): 96877 emails\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. SEPARATE NORMAL FROM PHISHING ---\n",
    "# Isolate Normal emails (label 0) for training and validation\n",
    "df_normal_only = df_combined[df_combined['is_phishing'] == 0].copy()\n",
    "\n",
    "# Isolate Phishing emails (label 1) for the Test set\n",
    "df_phishing_only = df_combined[df_combined['is_phishing'] == 1].copy()\n",
    "\n",
    "print(f\"Normal emails isolated: {len(df_normal_only)}\")\n",
    "print(f\"Phishing emails isolated: {len(df_phishing_only)}\")\n",
    "\n",
    "\n",
    "# --- 2. CREATE TRAIN / VALIDATION SPLIT (70/30) ---\n",
    "# We split the Normal data into a large training set and a remainder.\n",
    "df_train_normal, df_remainder = train_test_split(\n",
    "    df_normal_only, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Split the remainder into Validation and Test sets (e.g., 50/50 of the 30% remainder)\n",
    "df_val_normal, df_test_normal_subset = train_test_split(\n",
    "    df_remainder, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3. CREATE FINAL TEST SET (NORMAL + ALL PHISHING) ---\n",
    "# Combine the normal subset with ALL the phishing attacks to create the final test set.\n",
    "# This is crucial for accurately measuring your Detection Rate (Recall).\n",
    "df_test_all = pd.concat([df_test_normal_subset, df_phishing_only], ignore_index=True)\n",
    "\n",
    "# Final step: Shuffle the test set so the model doesn't see all phishing at the end\n",
    "df_test_all = df_test_all.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- FINAL OUTPUT SUMMARY ---\n",
    "print(\"\\n--- Final Dataset Sizes ---\")\n",
    "print(f\"Training Set (100% Normal): {len(df_train_normal)} emails\")\n",
    "print(f\"Validation Set (100% Normal): {len(df_val_normal)} emails\")\n",
    "print(f\"Test Set (Normal + Phishing): {len(df_test_all)} emails\")\n",
    "\n",
    "# Your data is now ready for modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdb3e3b7-22e0-4aaa-88a0-2121acf9c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split datasets saved successfully. Ready to start the EDA.\n"
     ]
    }
   ],
   "source": [
    "# Assuming the splitting code ran successfully and these DataFrames exist:\n",
    "\n",
    "# 1. Save the final datasets\n",
    "df_train_normal.to_csv('01_train_normal.csv', index=False)\n",
    "df_val_normal.to_csv('02_val_normal.csv', index=False)\n",
    "df_test_all.to_csv('03_test_all.csv', index=False)\n",
    "\n",
    "print(\"Split datasets saved successfully. Ready to start the EDA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78def954-73cc-4fc8-9903-1cd11342a1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
